{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_rmt import RMTEncoderForSequenceClassification\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, DistributedSampler, Dataset\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, HfArgumentParser, AutoModel\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "config = {\n",
    "    'num_mem_tokens': 10,\n",
    "    'input_size': 512,\n",
    "    # 'input_seg_size': args.input_seg_size,\n",
    "    'model_attr': 'deberta',\n",
    "    # 'backbone_cls': backbone_cls,\n",
    "    'bptt_depth': -1, \n",
    "    'pad_token_id': 0,\n",
    "    'cls_token_id': tokenizer.cls_token_id, \n",
    "    'sep_token_id': tokenizer.sep_token_id,\n",
    "    'eos_token_id': 102,\n",
    "    \"data_path\": \"data/test.jsonl\",\n",
    "    \"batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"data_n_workers\": 1,\n",
    "}\n",
    "labels_map = {'false': 0, 'true': 1}\n",
    "encode_plus_kwargs = {'max_length': config[\"input_size\"],\n",
    "                              'truncation': True,\n",
    "                              'padding': 'longest',\n",
    "                              'pad_to_multiple_of': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperpartisanDataset(Dataset):\n",
    "    def __init__(self, datafile, x_field='text', label_field='label'):\n",
    "        if isinstance(datafile, str):\n",
    "            # convert str path to folder to Path\n",
    "            datafile = Path(datafile)\n",
    "        self.data = []\n",
    "        for line in datafile.open('r'):\n",
    "            self.data += [json.loads(line)]\n",
    "        self.x_field = x_field\n",
    "        self.label_field = label_field\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][self.x_field]\n",
    "        label = self.data[idx][self.label_field]\n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs, labels = zip(*batch)\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), return_tensors='pt', **encode_plus_kwargs)\n",
    "    labels = np.array([labels_map[t] for t in labels])\n",
    "    labels = {'labels': torch.from_numpy(labels)}\n",
    "    return {**features, **labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "load_path = \"deberta/run_1/model_best.pth\"\n",
    "checkpoint = torch.load(load_path, map_location='cuda')\n",
    "model = RMTEncoderForSequenceClassification.from_pretrained(model_name)\n",
    "model.set_params(num_mem_tokens=10, \n",
    "                input_size=config[\"input_size\"],\n",
    "                # input_seg_size=1002,\n",
    "                model_attr=config[\"model_attr\"],\n",
    "                # backbone_cls=transformers.BartForConditionalGeneration,\n",
    "                bptt_depth=-1, \n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                cls_token_id=tokenizer.cls_token_id, \n",
    "                sep_token_id=tokenizer.sep_token_id,\n",
    "                eos_token_id=config[\"eos_token_id\"],)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)\n",
    "model.cuda()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_worker_batch_size = config[\"batch_size\"] * config[\"gradient_accumulation_steps\"]\n",
    "kwargs = {'pin_memory': True, 'num_workers': config[\"data_n_workers\"]}\n",
    "test_data_path = Path(config[\"data_path\"]).expanduser().absolute()\n",
    "test_dataset = HyperpartisanDataset(test_data_path)\n",
    "test_sampler = DistributedSampler(test_dataset, shuffle=False, rank=0, num_replicas=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=per_worker_batch_size, sampler=test_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_dataloader:\n",
    "    pred = model(**batch,\n",
    "        # input_ids=batch[\"input_ids\"].cuda(), \n",
    "        # token_type_ids=batch[\"token_type_ids\"].cuda(),\n",
    "        # attention_mask=batch[\"attention_mask\"].cuda(), \n",
    "        # labels=batch[\"labels\"].cuda(),\n",
    "        output_attentions=True\n",
    "        )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 12, 512, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[\"attentions\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [tokenizer.convert_ids_to_tokens(t_id).replace('‚ñÅ', '') for t_id in batch[\"input_ids\"][0].tolist()]\n",
    "attentions = [layer_atts[0].detach().squeeze().numpy() if layer_atts[0] is not None else None for layer_atts in pred['encoder_attentions']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ff9e7b7d211a86c99ea06c6955b2b5efc52e00f77e6cf83b265a9066595fdd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
